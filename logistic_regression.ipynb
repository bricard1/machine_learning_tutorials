{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Prepared by: Benjamin Ricard, QBS108 TA | Last update: 12/3/2018\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this tutorial, students should be able to:\n",
    "\n",
    "* Understand the theoretical and mathematical basis for logistic regression\n",
    "\n",
    "### General Procedure\n",
    "\n",
    "* Simulate data that can be classified according to logistic regresssion\n",
    "* Analyze coeffecients and model output to understand model performance\n",
    "\n",
    "# Background\n",
    "\n",
    " \n",
    "Logistic regression represnts a regression that deviates from classical linear regression through incorporation of a logit link function. A link function is a general function that connects a linear regression output to a new form - here, from an unbounded linear function to a sigmoid with range [0,1].\n",
    "\n",
    " \n",
    "Starting from the definition of regression, the log-odds or logit link function (prob. of i / prob. not i) can be defined via:\n",
    " $$",
    "\\beta_0 x_0 + \\beta_1 x_1 + ... + \\beta_n x_n = logit(p_i(x)) = ln(\\frac{p_i(x)}{1-p_i(x)})\n",
    "$$",
    "\n",
    "For a basic 2 class classification, we can simplify this equation to find the probability:\n",
    " \n",
    " \n",
    "\n",
    " $$\n",
    "p_i = \\frac{e^{\\beta_0 x_0 + \\beta_1 x_1 + ... + \\beta_n x_n}}{e^{\\beta_0 x_0 + \\beta_1 x_1 + ... + \\beta_n x_n}+1} = \\frac{1}{e^{-(\\beta_0 x_0 + \\beta_1 x_1 + ... + \\beta_n x_n)}}\n",
    "$$\n",
    "\n",
    "With, accordingly, $1-p_i$ representing the probability of *not* being class $i$. Plotting of the general shape of an equation of the form $y = \\frac{1}{1+e^{-x}}$ shows a sigmoid that ranges in value between [0,1], and intepretation of the output of logistic regression can be accomplished by predicting class for an output $y_i$ based on probability. Usually, the rounded version of the probability can be used to determine predicted output. When training your own logistic regression model, care must be taken to carefully separate the predicted probabilities from classes. Minimizing differences in predicted probabilities vs predicted class can lead to very different results!\n",
    "\n",
    "# Simulations\n",
    "\n",
    " Simulate 6 informative coefficients and 94 noisy labels. 3 infomative coeffecients random numbers from [0,1], 3 informative coeffecients random numbers from [-1,0], and 94 labels from [-1,1]. The *true* output generated by the summation of the 6 informative coeffecients; 1 if sum >=0, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import random\n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "emp=[]\n",
    "for i in range(0,1001):\n",
    "    temp=[]\n",
    "    temp.append(random.uniform(0, 1))\n",
    "    temp.append(random.uniform(0, 1))\n",
    "    temp.append(random.uniform(0, 1))\n",
    "    temp.append(random.uniform(-1, 0))\n",
    "    temp.append(random.uniform(-1, 0))\n",
    "    temp.append(random.uniform(-1, 0))\n",
    "    for j in range(0,100-6):\n",
    "        temp.append(random.uniform(-1, 1))\n",
    "    emp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack=numpy.vstack(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate training/Test data\n",
    "Generate output according to informative coeffecients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=stack[0:500]\n",
    "testing=stack[501:1001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in range(0,len(emp)):\n",
    "    output.append(sum(emp[i][0:6]))\n",
    "binary=[]\n",
    "for i in output:\n",
    "    if i>=0:\n",
    "        binary.append(1)\n",
    "    else:\n",
    "        binary.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=training\n",
    "X_test=testing\n",
    "Y_train=binary[0:500]\n",
    "Y_test = binary[501:1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can solve the MLE of the above logit-equation (and incorporate regularization), but you can also just use Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Testing Accuracy:  0.898\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,Y_train)\n",
    "print('Mean Testing Accuracy: ', clf.score(X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we made it so only the first 6 coefficients were informative - we expect that a good model would be able to find these informative features by weighing these variables more strongly. We see this is the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 Coefficients:  [ 4.23691694  3.46056115  3.36118641  3.97574661  3.5723408   3.8072048\n",
      " -0.3320955   0.08455914  0.10260776 -0.32276413  0.02081938 -0.02323587\n",
      " -0.0667668   0.03450831  0.16773663]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 15 Coefficients: \",clf.coef_[0][0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Testing Predictions: \n",
      "[Prob = 1 Prob = 0]\n",
      "[[0.02172683 0.97827317]\n",
      " [0.98085023 0.01914977]\n",
      " [0.00691872 0.99308128]\n",
      " [0.21911226 0.78088774]\n",
      " [0.01106238 0.98893762]\n",
      " [0.88934149 0.11065851]\n",
      " [0.05136115 0.94863885]\n",
      " [0.96333054 0.03666946]\n",
      " [0.25407114 0.74592886]\n",
      " [0.46572383 0.53427617]]\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 Testing Predictions: \")\n",
    "print(\"[Prob = 1\", \"Prob = 0]\")      \n",
    "print(clf.predict_proba(X_test)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Testing Labels:  [1, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"True Testing Labels: \",Y_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model works well, as evidenced by the predicted vs true outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
